{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个文件主要是根据法律的常识(而非仅仅依靠语言和逻辑的理解), 去补充MEU之间的关系 (and, or, not). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils.call_gpt import call_gpt, call_gpt_async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用LLM获取MEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_meu_inner_relation_v1 = \"\"\"\n",
    "# MEU关系识别指令 inner relation \"or\"\n",
    "\n",
    "## 角色定位\n",
    "你是一个资深法律条款引用分析专家，专注识别同一个法条下若干 MEU (法律的最小可执行单元) 之间的 \"or\" 关系\n",
    "\n",
    "## MEU概念简述\n",
    "MEU（Minimum Executable Unit）是法律条文拆解出的最小合规单元，包含：\n",
    "- MEU_id: MEU的编号, 通常为\"MEU_n_k\", 其中n是其所属的法条的编号, k是其在法条内部的编号. 第k个法条编号为Law_k. \n",
    "- subject: 责任主体（如\"控股股东\"）\n",
    "- condition: 触发条件（如\"减持股份\"） \n",
    "- constraint: 约束内容（如\"提前15日公告\"）\n",
    "- contextual_info: 补充说明（如价格计算方式）\n",
    "我们采用MEU判断案例的合规性时, 会先检查案例主体是否符合MEU主体, 再检查案例中的条件是否符合MEU中的条件, 当前两者都满足, 再检查案例中的主体的行为是否违反了MEU中的约束. 只有主体, 条件和约束全部满足, 才会认为该案例在该MEU上违规, 负责会判定该案例在该MEU上不违规. \n",
    "\n",
    "\n",
    "## 核心任务\n",
    "从给定的MEU列表中识别 or 关系，输出(source_id, or, target_id)三元组. \n",
    "当target_id有多个时可以用列表承载, 例如(source_id, or, [target_id_1, target_id_2])\n",
    "当所给的MEU之间或者MEU与法律之间不存在or时, 如实返回空值, 不需要自己杜撰或强行拼凑or. MEU间不存在关系是常见的现象. \n",
    "\n",
    "\n",
    "## or关系的含义与注意事项\n",
    "- or关系的含义: MEU之间默认的关系是and, 也即只有当整个法条Law_k下全部MEU都不违规, 这个法条才会被判定为不违规. 但有时这种简单的结构无法涵盖法律的本意, 需要我们判断是否存在 or 的情况. 如果两个MEU被判定为 or 的关系, 那么只要这两个MEU之间有一个不违规, 这两个MEU便会一起被判定为不违规. \n",
    "- 例子:\n",
    "法条原文: \n",
    "    \"第五条 上市公司大股东、董监高应当在股份减持计划实施完毕或者披露的减持时间区间届满后，及时向本所报告并披露减持结果公告。...\"\n",
    "拆分得MEU:\n",
    "    [{{\n",
    "    \"MEU_id\": \"MEU_5_1\",\n",
    "    \"subject\": \"上市公司大股东 | 董监高\",\n",
    "    \"condition\": \"股份减持计划实施完毕\",\n",
    "    \"constrain\": \"应当及时向本所报告并披露减持结果公告\",\n",
    "    \"contextual_info\": \"\",\n",
    "    }},{{\n",
    "    \"MEU_id\": \"MEU_5_2\",\n",
    "    \"subject\": \"上市公司大股东 | 董监高\",\n",
    "    \"condition\": \"披露的减持时间区间届满\",\n",
    "    \"constrain\": \"应当及时向本所报告并披露减持结果公告\",\n",
    "    \"contextual_info\": \"\",\n",
    "    }},...]\n",
    "得到关系:\n",
    "    思考: 虽然从语义和逻辑上看, 原文确实是在股份减持计划实施完毕和披露的减持时间区间届满这两种情况下, 都应当向本所报告并披露减持结果公告, 但考虑到业务逻辑, 单一的减持计划应当只需要披露一次减持结果公告. 因此, MEU_5_1和MEU_5_2不适用默认的\"and\"关系(否则减持计划实施完毕时披露了减持结果, 而减持时间区间届满没有再披露一次的案例会被误分类为违约), 而应当采用\"or\"的关系. \n",
    "    输出:\n",
    "    <RELATIONS>\n",
    "    [\n",
    "    (\"MEU_5_1\", \"or\", \"MEU_5_2\"),\n",
    "    ]\n",
    "    </RELATIONS>\n",
    "\n",
    "\n",
    "## 更多经验\n",
    "- \"or\" 的关系是双向的, 因此对于(\"MEU_n_i\", \"or\", \"MEU_n_j\")只需要输出一次即可, 无需再输出对向的(\"MEU_n_j\", \"or\", \"MEU_n_i\")\n",
    "- 并非法律原文中所有的\"或者\"都会构成MEU之间的or关系. 例如, condition中出现或的\"当主体A在面临情况B或C时, 应当遵守约束D\", 可以平行拆开为两个MEU[主体A面临情况B应当遵守D, 主体A面临情况C应当遵守D], 此时这两个MEU是默认的and关系. 但如果从业务的角度理解,  \"遵守D\"只需要发生一次, 主体A在面临B时进行D或者面临C时进行D, 只一次就好, 那么这两个拆开后的MEU就是or的关系. \n",
    "- 从业务的角度出发进行理解, 而不是从语言和逻辑的角度理解. 例如, \"第十五条 上市公司收到上交所或深交所受理或者不受理、中止或者终止审核、同意转板申请相关文书后，应当及时予以披露。\", 很明显这个法条要求在遇到受理, 不受理, 中止, 终止审核, 同意转板申请等相关文书后, 都要进行披露, 这个法条产生的MEU也就可以被默认的and关系表示, 不需要设置or的关系. \n",
    "\n",
    "\n",
    "## 识别原则\n",
    "1. 不修改MEU内容，仅建立关联\n",
    "2. 请遵循奥卡姆剃刀原则, 不要增加relation, 除非它是必要的.\n",
    "\n",
    "## 输出格式\n",
    "用<RELATIONS>标签包裹的 Python 列表, 列表内为一个个relation元组, 不要有任何注释等赘余内容. 下面是一个输出的样例: \n",
    "<RELATIONS>\n",
    "[\n",
    "(\"MEU_n_i\", \"or\", \"MEU_n_j\"),\n",
    "(\"MEU_n_k\", \"or\", \"MEU_n_r\"),\n",
    "]\n",
    "</RELATIONS>\n",
    "\n",
    "\n",
    "# 接下来是作为参考的法律原文, 和等待你发掘关系的MEU列表:\n",
    "\n",
    "## 法律原文\n",
    "{law_article}\n",
    "\n",
    "## MEU列表\n",
    "{MEU_list}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量调用llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils.call_gpt import call_gpt_async\n",
    "import os\n",
    "\n",
    "async def process_law_articles_async(\n",
    "    file_name,\n",
    "    max_concurrency: int = 5,\n",
    "    encoding: str = 'utf-8-sig'\n",
    "):\n",
    "    \"\"\"\n",
    "    异步处理法律条文的函数，支持并行控制\n",
    "    \n",
    "    参数：\n",
    "    input_file: 输入文件路径\n",
    "    output_file: 输出文件路径\n",
    "    max_concurrency: 最大并行请求数 (默认5)\n",
    "    encoding: 文件编码 (默认utf-8-sig)\n",
    "    \"\"\"\n",
    "\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        pass\n",
    "    else:\n",
    "        file_name += \".csv\"\n",
    "\n",
    "    input_dir_law = r\"law_to_MEU/st_1_law_csv\"\n",
    "    input_dir_MEU = r\"law_to_MEU/st_3_0_MEU/GT\"\n",
    "    output_dir = r\"law_to_MEU/st_3_1_inner_relations/raw_response\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    input_path_law = os.path.join(input_dir_law, file_name)\n",
    "    input_path_MEU = os.path.join(input_dir_MEU, file_name)\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # 读取CSV文件\n",
    "    with open(input_path_law, mode='r', encoding=encoding) as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        law_articles = list(reader)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    with open(input_path_MEU, mode='r', encoding=encoding) as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        MEU_list_all = list(reader)\n",
    "    \n",
    "    # 读取MEU数据并结构化分组\n",
    "    meu_dict = {}\n",
    "    for meu in MEU_list_all:\n",
    "        # 解析MEU_id结构\n",
    "        # 格式验证（MEU_n_k）\n",
    "        prefix, law_num, meu_num = meu[\"MEU_id\"].split(\"_\")\n",
    "        if prefix != \"MEU\" or not law_num.isdigit():\n",
    "            continue\n",
    "\n",
    "        # 初始化字典条目\n",
    "        if law_num not in meu_dict:\n",
    "            meu_dict[law_num] = []\n",
    "        \n",
    "        # 创建易读格式\n",
    "        formatted_meu = (\n",
    "            f\"MEU ID: {meu['MEU_id']}\\n\"\n",
    "            f\"Subject: {meu['subject']}\\n\"\n",
    "            f\"Condition: {meu['condition']}\\n\"\n",
    "            f\"Constraint: {meu['constraint']}\\n\"\n",
    "            f\"Contextual Info: {meu['contextual_info']}\"\n",
    "        )\n",
    "        meu_dict[law_num].append(formatted_meu)\n",
    "\n",
    "    # 共享状态容器\n",
    "    class State:\n",
    "        def __init__(self):\n",
    "            self.total_prompt_tokens = 0\n",
    "            self.total_completion_tokens = 0\n",
    "            self.success_count = 0\n",
    "            self.failure_count = 0\n",
    "            self.processed_data = []\n",
    "            self.lock = asyncio.Lock()\n",
    "\n",
    "    state = State()\n",
    "\n",
    "    async def process_article(article):\n",
    "        \"\"\"处理单个法条的异步任务\"\"\"\n",
    "        nonlocal state\n",
    "        law_article_num = article['law_article_num']\n",
    "        law_article = article['law_article']\n",
    "        \n",
    "        \n",
    "        try:\n",
    "\n",
    "            # 获取当前法条对应的MEU列表（空列表作为默认值）\n",
    "            # 获取结构化MEU数据\n",
    "            related_meu = meu_dict.get(law_article_num, [])\n",
    "            # 转换为JSON字符串（保持缩进格式）\n",
    "            MEU_list = json.dumps(related_meu, ensure_ascii=False, indent=2)\n",
    "\n",
    "            prompt = prompt_meu_inner_relation_v1.format(\n",
    "                law_article=law_article,\n",
    "                MEU_list=MEU_list,\n",
    "            )\n",
    "            \n",
    "            # 调用异步接口\n",
    "            content, reasoning_content, api_usage = await call_gpt_async(\n",
    "                prompt=prompt,\n",
    "                api_key=\"35684824-1776-48b6-94fd-96c2e99d0724\",\n",
    "                base_url=\"https://ark.cn-beijing.volces.com/api/v3\",\n",
    "                model=\"ep-20250217153824-9xcbx\",\n",
    "                temperature=0.6,\n",
    "            )\n",
    "            \n",
    "            # 原子操作更新状态\n",
    "            async with state.lock:\n",
    "                state.total_prompt_tokens += api_usage.prompt_tokens\n",
    "                state.total_completion_tokens += api_usage.completion_tokens\n",
    "                state.success_count += 1\n",
    "                state.processed_data.append({\n",
    "                    'law_article_num': law_article_num,\n",
    "                    'law_article': law_article,\n",
    "                    'response': content,\n",
    "                    'reasoning_content': reasoning_content,\n",
    "                    'api_usage': api_usage\n",
    "                })\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            async with state.lock:\n",
    "                state.failure_count += 1\n",
    "                print(f\"Failed to process law article {law_article_num}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # 创建异步任务\n",
    "    tasks = [process_article(article) for article in law_articles]\n",
    "    \n",
    "    # 使用tqdm进度条分批执行\n",
    "    pbar = tqdm_asyncio(total=len(tasks), desc=\"Processing law articles\")\n",
    "    \n",
    "    # 分批执行控制并发\n",
    "    for i in range(0, len(tasks), max_concurrency):\n",
    "        batch_tasks = tasks[i:i + max_concurrency]\n",
    "        await asyncio.gather(*batch_tasks)\n",
    "        pbar.update(len(batch_tasks))\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "    # 保存结果到CSV\n",
    "    with open(output_path, mode='w', encoding=encoding, newline='') as outfile:\n",
    "        fieldnames = ['law_article_num', 'law_article', 'response', 'reasoning_content', 'api_usage']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for data in state.processed_data:\n",
    "            writer.writerow(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await process_law_articles_async(\"北京证券交易所上市公司持续监管指引第7号——转板.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从LLM回复中提取MEU relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_relations_from_responses(file_name):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        pass\n",
    "    else:\n",
    "        file_name += \".csv\"\n",
    "\n",
    "    input_dir = r\"law_to_MEU/st_3_1_inner_relations/raw_response\"\n",
    "    output_dir = r\"law_to_MEU/st_3_1_inner_relations\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    def clean_response(response):\n",
    "        \"\"\"清理和解析response列中的关系数据\"\"\"\n",
    "        try:\n",
    "            # 统一处理转义符号\n",
    "            response = response.replace(\"<\\\\RELATIONS>\", \"</RELATIONS>\")\n",
    "            response = response.replace(\"<\\\\\\\\RELATIONS>\", \"</RELATIONS>\")\n",
    "            \n",
    "            # 提取所有RELATIONS标签内容\n",
    "            matches = re.findall(r'<RELATIONS>(.*?)</RELATIONS>', response, re.DOTALL)\n",
    "            \n",
    "            if matches:\n",
    "                # 选择内容最长的匹配项\n",
    "                longest_content = max(matches, key=lambda x: len(x.strip())).strip()\n",
    "                \n",
    "                # 数据格式转换\n",
    "                longest_content = longest_content.replace('\"\"', '\"')  # 处理双引号转义\n",
    "                longest_content = longest_content.replace('(', '[').replace(')', ']')  # 转换括号格式\n",
    "                \n",
    "                # 解析为JSON数组\n",
    "                return json.loads(longest_content)\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析错误: {e}\")\n",
    "            print(f\"问题内容: {longest_content}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"数据处理异常: {e}\")\n",
    "            return []\n",
    "\n",
    "    # 读取原始CSV文件\n",
    "    with open(input_path, mode='r', encoding='utf-8-sig') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        law_articles = list(reader)\n",
    "\n",
    "    # 处理关系数据\n",
    "    split_data = []\n",
    "    fieldnames = ['source', 'relation', 'target']\n",
    "    \n",
    "    for row in law_articles:\n",
    "        relations = clean_response(row['response'])\n",
    "        for rel_entry in relations:\n",
    "            # 校验数据格式\n",
    "            if not isinstance(rel_entry, list) or len(rel_entry) != 3:\n",
    "                print(f\"异常条目: {rel_entry}\")\n",
    "                continue\n",
    "            \n",
    "            source, relation, targets = rel_entry\n",
    "            # 处理target为列表的情况\n",
    "            if isinstance(targets, list):\n",
    "                for target in targets:\n",
    "                    split_data.append({\n",
    "                        'source': source.strip('\"'),  # 去除可能的残留引号\n",
    "                        'relation': relation.strip('\"'),\n",
    "                        'target': target.strip('\"')\n",
    "                    })\n",
    "            else:\n",
    "                split_data.append({\n",
    "                    'source': source.strip('\"'),\n",
    "                    'relation': relation.strip('\"'),\n",
    "                    'target': targets.strip('\"')\n",
    "                })\n",
    "\n",
    "    # 写入处理后的CSV文件\n",
    "    with open(output_path, mode='w', encoding='utf-8-sig', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(split_data)\n",
    "    \n",
    "    print(f'文件已保存至: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_relations_from_responses(\"北京证券交易所上市公司持续监管指引第4号——股份回购.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京证券交易所上市公司持续监管指引第8号——股份减持和持股管理.csv',\n",
       " '北京证券交易所上市公司持续监管指引第5号——要约收购.csv',\n",
       " '北京证券交易所上市公司持续监管指引第1号——独立董事.csv',\n",
       " '北京证券交易所上市公司持续监管指引第4号——股份回购.csv',\n",
       " '北京证券交易所上市公司持续监管指引第10号——权益分派.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# 定义文件目录路径\n",
    "directory_path = r\"law_to_MEU/st_3_0_MEU/GT\"\n",
    "\n",
    "# 提取所有.doc文件的文件名，不包含路径\n",
    "filenames = [\n",
    "    f for f in os.listdir(directory_path) if f.endswith('.csv')\n",
    "]\n",
    "\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已保存至: law_to_MEU/st_3_1_inner_relations/北京证券交易所上市公司持续监管指引第8号——股份减持和持股管理.csv\n",
      "文件已保存至: law_to_MEU/st_3_1_inner_relations/北京证券交易所上市公司持续监管指引第5号——要约收购.csv\n",
      "文件已保存至: law_to_MEU/st_3_1_inner_relations/北京证券交易所上市公司持续监管指引第1号——独立董事.csv\n",
      "文件已保存至: law_to_MEU/st_3_1_inner_relations/北京证券交易所上市公司持续监管指引第4号——股份回购.csv\n",
      "文件已保存至: law_to_MEU/st_3_1_inner_relations/北京证券交易所上市公司持续监管指引第10号——权益分派.csv\n"
     ]
    }
   ],
   "source": [
    "# for doc_file in doc_filenames:\n",
    "#     main(doc_file)\n",
    "\n",
    "for filename in filenames:\n",
    "    # await process_law_articles_async(filename)\n",
    "    get_relations_from_responses(filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GE311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
